{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8cd2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->wikipedia) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/eshanaagarwal/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a1bfe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<WikipediaPage 'Decentralized finance'>, <WikipediaPage '0x (decentralized exchange infrastructure)'>, <WikipediaPage 'Decentralized application'>, <WikipediaPage 'Decentralized computing'>, <WikipediaPage 'Decentralized network 42'>, <WikipediaPage 'Decentralized autonomous organization'>, <WikipediaPage 'Decentralization'>, <WikipediaPage 'Definition of anarchism and libertarianism'>, <WikipediaPage 'Definitions of fascism'>, <WikipediaPage 'Definition of terrorism'>, <WikipediaPage 'Decentralised system'>]\n"
     ]
    }
   ],
   "source": [
    "# EXTRACTS THE DATA FROM WIKIPEDIA (we can change this to telegram or )\n",
    "\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "\n",
    "\n",
    "def filter(titles):\n",
    "    \"\"\"\n",
    "    Get the titles which are related to curve wars, given a list of titles\n",
    "    \"\"\"\n",
    "    titles = [title for title in titles if 'defi' in title.lower() or 'decentralize' in title.lower()]\n",
    "\n",
    "    return titles\n",
    "\n",
    "\n",
    "def get_wiki_page(title):\n",
    "    \"\"\"\n",
    "    Get the wikipedia page given a title\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return wikipedia.page(title)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return wikipedia.page(e.options[0])\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def recursively_find_all_pages(titles, titles_so_far=set()):\n",
    "    \"\"\"\n",
    "    Recursively find all the pages that are linked to the Wikipedia titles in the list\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "\n",
    "    titles = list(set(titles) - titles_so_far)\n",
    "    titles = filter(titles)\n",
    "    titles_so_far.update(titles)\n",
    "    for title in titles:\n",
    "        page = get_wiki_page(title)\n",
    "        if page is None:\n",
    "            continue\n",
    "        all_pages.append(page)\n",
    "\n",
    "        new_pages = recursively_find_all_pages(page.links, titles_so_far)\n",
    "        for pg in new_pages:\n",
    "            if pg.title not in [p.title for p in all_pages]:\n",
    "                all_pages.append(pg)\n",
    "        titles_so_far.update(page.links)\n",
    "    return all_pages\n",
    "\n",
    "pages = recursively_find_all_pages([\"Decentralized Finance\"])\n",
    "print((pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fadaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: requests in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eshanaagarwal/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/eshanaagarwal/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b295037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING THE DATA INTO SECTIONS\n",
    "\n",
    "import re\n",
    "from typing import Set\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def reduce_long(\n",
    "    long_text: str, long_text_tokens: bool = False, max_len: int = 590\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reduce a long text to a maximum of `max_len` tokens by potentially cutting at a sentence end\n",
    "    \"\"\"\n",
    "    if not long_text_tokens:\n",
    "        long_text_tokens = count_tokens(long_text)\n",
    "    if long_text_tokens > max_len:\n",
    "        sentences = sent_tokenize(long_text.replace(\"\\n\", \" \"))\n",
    "        ntokens = 0\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            ntokens += 1 + count_tokens(sentence)\n",
    "            if ntokens > max_len:\n",
    "                return \". \".join(sentences[:i][:-1]) + \".\"\n",
    "\n",
    "    return long_text\n",
    "\n",
    "discard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",\n",
    "    \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",\n",
    "    \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",\n",
    "    \"References and notes\",]\n",
    "\n",
    "\n",
    "def extract_sections(\n",
    "    wiki_text: str,\n",
    "    title: str,\n",
    "    max_len: int = 1000,\n",
    "    discard_categories: Set[str] = discard_categories,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "        Extract the sections of a Wikipedia page, discarding the the references and other low information sections\n",
    "        \"\"\"\n",
    "    if len(wiki_text) == 0:\n",
    "        return []\n",
    "\n",
    "    # find all headings and the coresponding contents\n",
    "    headings = re.findall(\"==+ .* ==+\", wiki_text)\n",
    "    for heading in headings:\n",
    "        wiki_text = wiki_text.replace(heading, \"==+ !! ==+\")\n",
    "    contents = wiki_text.split(\"==+ !! ==+\")\n",
    "    contents = [c.strip() for c in contents]\n",
    "    assert len(headings) == len(contents) - 1\n",
    "\n",
    "    cont = contents.pop(0).strip()\n",
    "    outputs = [(title, \"Summary\", cont, count_tokens(cont) + 4)]\n",
    "\n",
    "    # discard the discard categories, accounting for a tree structure\n",
    "    max_level = 100\n",
    "    keep_group_level = max_level\n",
    "    remove_group_level = max_level\n",
    "    nheadings, ncontents = [], []\n",
    "    for heading, content in zip(headings, contents):\n",
    "        plain_heading = \" \".join(heading.split(\" \")[1:-1])\n",
    "        num_equals = len(heading.split(\" \")[0])\n",
    "        if num_equals <= keep_group_level:\n",
    "            keep_group_level = max_level\n",
    "\n",
    "        if num_equals > remove_group_level:\n",
    "            if (\n",
    "                    num_equals <= keep_group_level\n",
    "            ):\n",
    "                continue\n",
    "        keep_group_level = max_level\n",
    "        if plain_heading in discard_categories:\n",
    "            remove_group_level = num_equals\n",
    "            keep_group_level = max_level\n",
    "            continue\n",
    "        nheadings.append(heading.replace(\"=\", \"\").strip())\n",
    "        ncontents.append(content)\n",
    "        remove_group_level = max_level\n",
    "\n",
    "    # count the tokens of each section\n",
    "    ncontent_ntokens = [\n",
    "        count_tokens(c)\n",
    "        + 3\n",
    "        + count_tokens(\" \".join(h.split(\" \")[1:-1]))\n",
    "        - (1 if len(c) == 0 else 0)\n",
    "        for h, c in zip(nheadings, ncontents)\n",
    "    ]\n",
    "\n",
    "    # Create a tuple of (title, section_name, content, number of tokens)\n",
    "    outputs += [(title, h, c, t) if t < max_len\n",
    "                else (title, h, reduce_long(c, max_len), count_tokens(reduce_long(c, max_len)))\n",
    "                for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d92fa348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/eshanaagarwal/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/share/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d6730a4334d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mextract_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"heading\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5e938edfdadc>\u001b[0m in \u001b[0;36mextract_sections\u001b[0;34m(wiki_text, title, max_len, discard_categories)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Create a tuple of (title, section_name, content, number of tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     outputs += [(title, h, c, t) if t < max_len\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n",
      "\u001b[0;32m<ipython-input-20-5e938edfdadc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Create a tuple of (title, section_name, content, number of tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     outputs += [(title, h, c, t) if t < max_len\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5e938edfdadc>\u001b[0m in \u001b[0;36mreduce_long\u001b[0;34m(long_text, long_text_tokens, max_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlong_text_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlong_text_tokens\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/eshanaagarwal/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/share/nltk_data'\n    - '/Users/eshanaagarwal/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for page in pages:\n",
    "    if not(extract_sections(page.content, page.title)):\n",
    "        \n",
    "    res += extract_sections(page.content, page.title)\n",
    "df = pd.DataFrame(res, columns=[\"title\", \"heading\", \"content\", \"tokens\"])\n",
    "df = df[df.tokens>40]\n",
    "df = df.drop_duplicates(['title','heading'])\n",
    "df = df.reset_index().drop('index',axis=1) # reset index\n",
    "df.head()\n",
    "df.to_csv('ad_hoc_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc451bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
